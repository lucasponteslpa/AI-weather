{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da72982f-4a9b-40e3-a08f-b4dc921b1e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 2])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class FullyConnectedModule(nn.Module):\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_hidden_layers: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # input layer\n",
    "        module_list = [nn.Linear(input_size, hidden_size), nn.ReLU()]\n",
    "        # hidden layers\n",
    "        for _ in range(n_hidden_layers):\n",
    "            module_list.extend([nn.Linear(hidden_size, hidden_size), nn.ReLU()])\n",
    "        # output layer\n",
    "        module_list.append(nn.Linear(hidden_size, output_size))\n",
    "\n",
    "        self.sequential = nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x of shape: batch_size x n_timesteps_in\n",
    "        # output of shape batch_size x n_timesteps_out\n",
    "        return self.sequential(x)\n",
    "\n",
    "\n",
    "# test that network works as intended\n",
    "network = FullyConnectedModule(input_size=5, output_size=2, hidden_size=10, n_hidden_layers=2)\n",
    "x = torch.rand(20, 5)\n",
    "network(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef137fe9-2a1a-4636-9aad-161d21ecd87f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpa1/anaconda3/envs/weather/lib/python3.10/site-packages/pytorch_forecasting/models/base_model.py:30: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Writing Pytorch Forecasting Model\n",
    "from typing import Dict\n",
    "\n",
    "from pytorch_forecasting.models import BaseModel\n",
    "\n",
    "\n",
    "class PFFullyConnectedModel(BaseModel):\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int, n_hidden_layers: int, **kwargs):\n",
    "        # saves arguments in signature to `.hparams` attribute, mandatory call - do not skip this\n",
    "        self.save_hyperparameters()\n",
    "        # pass additional arguments to BaseModel.__init__, mandatory call - do not skip this\n",
    "        super().__init__(**kwargs)\n",
    "        self.network = FullyConnectedModule(\n",
    "            input_size=self.hparams.input_size,\n",
    "            output_size=self.hparams.output_size,\n",
    "            hidden_size=self.hparams.hidden_size,\n",
    "            n_hidden_layers=self.hparams.n_hidden_layers,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        # x is a batch generated based on the TimeSeriesDataset\n",
    "        network_input = x[\"encoder_cont\"].squeeze(-1)\n",
    "        prediction = self.network(network_input)\n",
    "\n",
    "        # rescale predictions into target space\n",
    "        prediction = self.transform_output(prediction, target_scale=x[\"target_scale\"])\n",
    "\n",
    "        # We need to return a dictionary that at least contains the prediction\n",
    "        # The parameter can be directly forwarded from the input.\n",
    "        # The conversion to a named tuple can be directly achieved with the `to_network_output` function.\n",
    "        return self.to_network_output(prediction=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c7dd941-7c58-426b-bfcf-664f5cdc56ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Dataset\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "test_data = pd.read_csv('data/dados_ponto_recife.csv')\n",
    "time_p_group=32\n",
    "total_timesteps = np.arange(0,test_data.shape[0])\n",
    "test_data['timestep'] = total_timesteps%time_p_group\n",
    "test_data['group'] = total_timesteps//time_p_group\n",
    "training_cutoff = \"YYYY-MM-DD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def4e5d0-9cfa-4aa3-ab70-a1e9d514445f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpa1/anaconda3/envs/weather/lib/python3.10/site-packages/pytorch_forecasting/data/timeseries.py:1281: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 1 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__group': 23010}]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'time_idx': 'timestep',\n",
       " 'target': 'tp',\n",
       " 'group_ids': ['group'],\n",
       " 'weight': None,\n",
       " 'max_encoder_length': 30,\n",
       " 'min_encoder_length': 30,\n",
       " 'min_prediction_idx': 0,\n",
       " 'min_prediction_length': 2,\n",
       " 'max_prediction_length': 2,\n",
       " 'static_categoricals': [],\n",
       " 'static_reals': [],\n",
       " 'time_varying_known_categoricals': [],\n",
       " 'time_varying_known_reals': [],\n",
       " 'time_varying_unknown_categoricals': [],\n",
       " 'time_varying_unknown_reals': ['tp'],\n",
       " 'variable_groups': {},\n",
       " 'constant_fill_strategy': {},\n",
       " 'allow_missing_timesteps': False,\n",
       " 'lags': {},\n",
       " 'add_relative_time_idx': False,\n",
       " 'add_target_scales': False,\n",
       " 'add_encoder_length': False,\n",
       " 'target_normalizer': EncoderNormalizer(\n",
       " \tmethod='standard',\n",
       " \tcenter=True,\n",
       " \tmax_length=None,\n",
       " \ttransformation=None,\n",
       " \tmethod_kwargs={}\n",
       " ),\n",
       " 'categorical_encoders': {'__group_id__group': NaNLabelEncoder(add_nan=False, warn=True)},\n",
       " 'scalers': {},\n",
       " 'randomize_length': None,\n",
       " 'predict_mode': False}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "\n",
    "# create the dataset from the pandas dataframe\n",
    "dataset = TimeSeriesDataSet(\n",
    "    test_data[lambda x: x.time <= training_cutoff],\n",
    "    group_ids=[\"group\"],\n",
    "    target=\"tp\",\n",
    "    time_idx=\"timestep\",\n",
    "    min_encoder_length=30,\n",
    "    max_encoder_length=30,\n",
    "    min_prediction_length=2,\n",
    "    max_prediction_length=2,\n",
    "    time_varying_unknown_reals=[\"tp\"],\n",
    ")\n",
    "dataset.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de80f18a-5123-429c-b572-3815de6420b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = {'encoder_cat': tensor([], size=(64, 30, 0), dtype=torch.int64), 'encoder_cont': tensor([[[-1.1034],\n",
      "         [-0.8777],\n",
      "         [-0.0219],\n",
      "         ...,\n",
      "         [ 0.3083],\n",
      "         [-0.1499],\n",
      "         [-1.2315]],\n",
      "\n",
      "        [[ 2.2836],\n",
      "         [ 0.0376],\n",
      "         [-0.8560],\n",
      "         ...,\n",
      "         [-0.1154],\n",
      "         [-0.5421],\n",
      "         [-0.7755]],\n",
      "\n",
      "        [[-0.6656],\n",
      "         [-0.7025],\n",
      "         [-0.7075],\n",
      "         ...,\n",
      "         [ 0.6749],\n",
      "         [ 0.3587],\n",
      "         [ 0.2174]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.7809],\n",
      "         [-0.7301],\n",
      "         [-0.7042],\n",
      "         ...,\n",
      "         [-0.1790],\n",
      "         [ 0.5042],\n",
      "         [-0.3027]],\n",
      "\n",
      "        [[-0.3272],\n",
      "         [-0.0702],\n",
      "         [-0.8505],\n",
      "         ...,\n",
      "         [ 0.2648],\n",
      "         [-0.8551],\n",
      "         [-0.7908]],\n",
      "\n",
      "        [[-0.2047],\n",
      "         [-0.1620],\n",
      "         [ 1.9013],\n",
      "         ...,\n",
      "         [-0.4608],\n",
      "         [-0.3825],\n",
      "         [-0.5035]]]), 'encoder_target': tensor([[0.0139, 0.0266, 0.0749,  ..., 0.0935, 0.0677, 0.0067],\n",
      "        [0.1907, 0.0577, 0.0048,  ..., 0.0486, 0.0234, 0.0095],\n",
      "        [0.1578, 0.1364, 0.1335,  ..., 0.9360, 0.7524, 0.6704],\n",
      "        ...,\n",
      "        [0.0353, 0.0606, 0.0734,  ..., 0.3347, 0.6747, 0.2732],\n",
      "        [0.1659, 0.1926, 0.1116,  ..., 0.2275, 0.1111, 0.1178],\n",
      "        [0.0982, 0.1011, 0.2394,  ..., 0.0811, 0.0863, 0.0782]]), 'encoder_lengths': tensor([30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30,\n",
      "        30, 30, 30, 30, 30, 30, 30, 30, 30, 30]), 'decoder_cat': tensor([], size=(64, 2, 0), dtype=torch.int64), 'decoder_cont': tensor([[[-1.2719e+00],\n",
      "         [-1.3157e+00]],\n",
      "\n",
      "        [[-7.8356e-01],\n",
      "         [-8.2381e-01]],\n",
      "\n",
      "        [[-4.3558e-01],\n",
      "         [-2.9184e-01]],\n",
      "\n",
      "        [[ 1.6023e+00],\n",
      "         [ 3.1530e+00]],\n",
      "\n",
      "        [[ 1.1835e+00],\n",
      "         [-5.3869e-01]],\n",
      "\n",
      "        [[ 2.5344e-01],\n",
      "         [ 1.1898e+00]],\n",
      "\n",
      "        [[ 8.0458e-01],\n",
      "         [ 7.9422e-01]],\n",
      "\n",
      "        [[-4.9915e-01],\n",
      "         [ 6.1387e-01]],\n",
      "\n",
      "        [[ 8.3397e-01],\n",
      "         [ 2.4250e+00]],\n",
      "\n",
      "        [[-3.7030e-01],\n",
      "         [-4.4639e-01]],\n",
      "\n",
      "        [[-1.1935e+00],\n",
      "         [-1.3597e+00]],\n",
      "\n",
      "        [[-6.3448e-01],\n",
      "         [-6.4725e-01]],\n",
      "\n",
      "        [[-4.8397e-01],\n",
      "         [-3.2968e-01]],\n",
      "\n",
      "        [[-7.7713e-01],\n",
      "         [-7.5401e-01]],\n",
      "\n",
      "        [[ 4.4451e-01],\n",
      "         [ 2.6618e+00]],\n",
      "\n",
      "        [[-5.9410e-02],\n",
      "         [-5.1346e-01]],\n",
      "\n",
      "        [[-3.7439e-02],\n",
      "         [ 2.5913e-01]],\n",
      "\n",
      "        [[-8.5965e-01],\n",
      "         [-2.9582e-01]],\n",
      "\n",
      "        [[ 9.0409e-01],\n",
      "         [ 3.4248e+00]],\n",
      "\n",
      "        [[-3.9134e-01],\n",
      "         [ 5.8820e-01]],\n",
      "\n",
      "        [[-7.9655e-01],\n",
      "         [-1.1472e+00]],\n",
      "\n",
      "        [[-2.3707e-01],\n",
      "         [ 1.8391e-02]],\n",
      "\n",
      "        [[-7.3270e-01],\n",
      "         [-9.0691e-01]],\n",
      "\n",
      "        [[-2.8821e-01],\n",
      "         [ 1.9556e-01]],\n",
      "\n",
      "        [[ 3.3549e+00],\n",
      "         [ 3.3260e+00]],\n",
      "\n",
      "        [[-3.2521e-01],\n",
      "         [-9.0464e-02]],\n",
      "\n",
      "        [[-1.0372e+00],\n",
      "         [-9.0204e-01]],\n",
      "\n",
      "        [[-1.0229e+00],\n",
      "         [-1.0229e+00]],\n",
      "\n",
      "        [[-9.9936e-02],\n",
      "         [-3.8621e-01]],\n",
      "\n",
      "        [[ 1.6845e-01],\n",
      "         [ 7.0637e-01]],\n",
      "\n",
      "        [[-3.5486e-01],\n",
      "         [-4.7960e-01]],\n",
      "\n",
      "        [[ 1.5442e+00],\n",
      "         [ 2.8080e+00]],\n",
      "\n",
      "        [[-1.6616e-01],\n",
      "         [-3.1942e-01]],\n",
      "\n",
      "        [[ 1.4395e+00],\n",
      "         [ 1.1715e+00]],\n",
      "\n",
      "        [[-9.7952e-01],\n",
      "         [-9.7952e-01]],\n",
      "\n",
      "        [[-8.8179e-01],\n",
      "         [-8.7763e-01]],\n",
      "\n",
      "        [[ 2.2188e+00],\n",
      "         [ 1.1537e+00]],\n",
      "\n",
      "        [[-7.8837e-01],\n",
      "         [-7.9394e-01]],\n",
      "\n",
      "        [[-1.3293e+00],\n",
      "         [-1.3293e+00]],\n",
      "\n",
      "        [[ 1.1789e+00],\n",
      "         [ 4.1383e-01]],\n",
      "\n",
      "        [[-5.1478e-01],\n",
      "         [-5.0466e-01]],\n",
      "\n",
      "        [[ 7.8087e-02],\n",
      "         [ 3.2837e-01]],\n",
      "\n",
      "        [[-6.9497e-01],\n",
      "         [-6.9497e-01]],\n",
      "\n",
      "        [[-2.8174e-01],\n",
      "         [-4.8873e-01]],\n",
      "\n",
      "        [[ 1.4228e+00],\n",
      "         [ 3.4163e+00]],\n",
      "\n",
      "        [[ 3.2812e+00],\n",
      "         [ 6.3320e-01]],\n",
      "\n",
      "        [[-2.1589e-03],\n",
      "         [ 9.8573e-01]],\n",
      "\n",
      "        [[-8.6596e-01],\n",
      "         [-9.1501e-01]],\n",
      "\n",
      "        [[ 8.9929e-01],\n",
      "         [ 7.2731e-01]],\n",
      "\n",
      "        [[-8.7052e-01],\n",
      "         [-9.0140e-01]],\n",
      "\n",
      "        [[-2.2484e-01],\n",
      "         [ 8.5390e-01]],\n",
      "\n",
      "        [[-6.7755e-01],\n",
      "         [-6.9105e-01]],\n",
      "\n",
      "        [[-8.0092e-01],\n",
      "         [-8.0092e-01]],\n",
      "\n",
      "        [[ 6.4312e+00],\n",
      "         [ 1.5333e+01]],\n",
      "\n",
      "        [[-1.1142e+00],\n",
      "         [-1.1077e+00]],\n",
      "\n",
      "        [[ 2.0199e+00],\n",
      "         [ 1.3752e+00]],\n",
      "\n",
      "        [[ 2.2738e+00],\n",
      "         [ 3.3166e+00]],\n",
      "\n",
      "        [[-1.1335e+00],\n",
      "         [-1.0878e+00]],\n",
      "\n",
      "        [[-4.8208e-01],\n",
      "         [-5.5774e-01]],\n",
      "\n",
      "        [[ 1.6872e+00],\n",
      "         [ 4.6657e-01]],\n",
      "\n",
      "        [[-5.5448e-01],\n",
      "         [-4.5273e-01]],\n",
      "\n",
      "        [[-6.7353e-01],\n",
      "         [-7.9332e-01]],\n",
      "\n",
      "        [[ 5.8151e-01],\n",
      "         [ 2.2811e-01]],\n",
      "\n",
      "        [[-7.4538e-01],\n",
      "         [-1.1296e+00]]]), 'decoder_target': tensor([[4.3730e-03, 1.9010e-03],\n",
      "        [9.0600e-03, 6.6760e-03],\n",
      "        [2.9135e-01, 3.7479e-01],\n",
      "        [2.5940e-01, 4.0436e-01],\n",
      "        [1.3256e-01, 3.0041e-02],\n",
      "        [8.0585e-02, 1.4019e-01],\n",
      "        [8.8215e-02, 8.7738e-02],\n",
      "        [9.5370e-03, 8.2016e-02],\n",
      "        [1.2494e-01, 2.2082e-01],\n",
      "        [8.9169e-02, 8.1539e-02],\n",
      "        [9.4891e-02, 4.8637e-02],\n",
      "        [1.6262e-02, 1.4767e-02],\n",
      "        [2.2936e-01, 3.1567e-01],\n",
      "        [3.4809e-02, 3.6716e-02],\n",
      "        [1.7119e-01, 3.4094e-01],\n",
      "        [6.1989e-02, 1.7166e-02],\n",
      "        [2.0409e-01, 4.0913e-01],\n",
      "        [9.5100e-04, 3.3837e-02],\n",
      "        [1.3924e-01, 3.3999e-01],\n",
      "        [2.5654e-01, 7.4911e-01],\n",
      "        [3.9094e-02, 2.4260e-03],\n",
      "        [1.8358e-01, 3.5667e-01],\n",
      "        [2.6703e-02, 1.0490e-02],\n",
      "        [1.5211e-01, 3.9577e-01],\n",
      "        [1.3285e+00, 1.3199e+00],\n",
      "        [3.3855e-02, 4.5776e-02],\n",
      "        [2.6703e-02, 4.7207e-02],\n",
      "        [0.0000e+00, 0.0000e+00],\n",
      "        [1.0729e-01, 7.2956e-02],\n",
      "        [2.6130e-01, 3.8429e-01],\n",
      "        [7.4863e-02, 4.0531e-02],\n",
      "        [1.0920e-01, 1.6355e-01],\n",
      "        [9.1553e-02, 7.2479e-02],\n",
      "        [2.1648e-01, 1.9360e-01],\n",
      "        [0.0000e+00, 0.0000e+00],\n",
      "        [1.4310e-03, 1.9070e-03],\n",
      "        [3.0041e-01, 1.9073e-01],\n",
      "        [4.4400e-04, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00],\n",
      "        [2.7609e-01, 1.7691e-01],\n",
      "        [4.7700e-04, 1.4310e-03],\n",
      "        [6.8188e-02, 8.6784e-02],\n",
      "        [0.0000e+00, 0.0000e+00],\n",
      "        [4.2677e-01, 3.6335e-01],\n",
      "        [1.7166e-02, 3.3379e-02],\n",
      "        [4.3345e-01, 1.8358e-01],\n",
      "        [9.0122e-02, 1.7023e-01],\n",
      "        [1.3351e-02, 2.8610e-03],\n",
      "        [3.5763e-02, 3.1471e-02],\n",
      "        [2.6703e-02, 2.3365e-02],\n",
      "        [2.0218e-01, 2.8706e-01],\n",
      "        [9.5400e-04, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00],\n",
      "        [3.7003e-01, 8.3733e-01],\n",
      "        [0.0000e+00, 4.7700e-04],\n",
      "        [4.7875e-01, 3.7813e-01],\n",
      "        [5.6267e-02, 7.7248e-02],\n",
      "        [1.1197e-01, 1.3346e-01],\n",
      "        [6.9618e-02, 6.4373e-02],\n",
      "        [2.3680e+00, 1.5306e+00],\n",
      "        [9.1250e-03, 1.3877e-02],\n",
      "        [8.8692e-02, 2.9087e-02],\n",
      "        [2.6035e-01, 2.2364e-01],\n",
      "        [6.1989e-02, 3.6240e-02]]), 'decoder_lengths': tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'decoder_time_idx': tensor([[30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31],\n",
      "        [30, 31]]), 'groups': tensor([[ 1309],\n",
      "        [18055],\n",
      "        [18157],\n",
      "        [14425],\n",
      "        [ 6926],\n",
      "        [20535],\n",
      "        [11688],\n",
      "        [ 6869],\n",
      "        [ 3317],\n",
      "        [15843],\n",
      "        [17362],\n",
      "        [ 4114],\n",
      "        [ 6941],\n",
      "        [20889],\n",
      "        [14932],\n",
      "        [17746],\n",
      "        [ 5803],\n",
      "        [ 1331],\n",
      "        [ 5270],\n",
      "        [11259],\n",
      "        [ 1792],\n",
      "        [17574],\n",
      "        [16370],\n",
      "        [20842],\n",
      "        [22049],\n",
      "        [10479],\n",
      "        [21482],\n",
      "        [18643],\n",
      "        [13882],\n",
      "        [  634],\n",
      "        [20345],\n",
      "        [ 8412],\n",
      "        [10341],\n",
      "        [11431],\n",
      "        [16217],\n",
      "        [ 6912],\n",
      "        [17965],\n",
      "        [ 3301],\n",
      "        [18760],\n",
      "        [18765],\n",
      "        [18365],\n",
      "        [10328],\n",
      "        [15080],\n",
      "        [17096],\n",
      "        [21366],\n",
      "        [18519],\n",
      "        [ 6485],\n",
      "        [19237],\n",
      "        [ 4862],\n",
      "        [11616],\n",
      "        [22360],\n",
      "        [14257],\n",
      "        [ 5678],\n",
      "        [22857],\n",
      "        [21731],\n",
      "        [14808],\n",
      "        [ 5725],\n",
      "        [ 4207],\n",
      "        [12242],\n",
      "        [10554],\n",
      "        [ 1208],\n",
      "        [18964],\n",
      "        [15904],\n",
      "        [14689]]), 'target_scale': tensor([[0.0761, 0.0564],\n",
      "        [0.0555, 0.0592],\n",
      "        [0.5442, 0.5805],\n",
      "        [0.1096, 0.0935],\n",
      "        [0.0621, 0.0595],\n",
      "        [0.0645, 0.0637],\n",
      "        [0.0512, 0.0460],\n",
      "        [0.0420, 0.0651],\n",
      "        [0.0747, 0.0603],\n",
      "        [0.1263, 0.1003],\n",
      "        [0.4271, 0.2783],\n",
      "        [0.0906, 0.1171],\n",
      "        [0.5001, 0.5594],\n",
      "        [0.0989, 0.0825],\n",
      "        [0.1372, 0.0766],\n",
      "        [0.0679, 0.0987],\n",
      "        [0.2300, 0.6914],\n",
      "        [0.0511, 0.0583],\n",
      "        [0.0672, 0.0796],\n",
      "        [0.4533, 0.5029],\n",
      "        [0.1224, 0.1046],\n",
      "        [0.3442, 0.6776],\n",
      "        [0.0949, 0.0931],\n",
      "        [0.2973, 0.5037],\n",
      "        [0.3353, 0.2960],\n",
      "        [0.0504, 0.0508],\n",
      "        [0.1841, 0.1517],\n",
      "        [0.0850, 0.0831],\n",
      "        [0.1193, 0.1199],\n",
      "        [0.2228, 0.2286],\n",
      "        [0.1725, 0.2752],\n",
      "        [0.0428, 0.0430],\n",
      "        [0.1122, 0.1245],\n",
      "        [0.0935, 0.0854],\n",
      "        [0.0346, 0.0353],\n",
      "        [0.1024, 0.1145],\n",
      "        [0.0719, 0.1030],\n",
      "        [0.0633, 0.0797],\n",
      "        [0.1994, 0.1500],\n",
      "        [0.1233, 0.1296],\n",
      "        [0.0490, 0.0943],\n",
      "        [0.0624, 0.0743],\n",
      "        [0.0033, 0.0048],\n",
      "        [0.5131, 0.3064],\n",
      "        [0.0056, 0.0081],\n",
      "        [0.1238, 0.0944],\n",
      "        [0.0903, 0.0811],\n",
      "        [0.1986, 0.2139],\n",
      "        [0.0133, 0.0250],\n",
      "        [0.1208, 0.1081],\n",
      "        [0.2199, 0.0787],\n",
      "        [0.0488, 0.0707],\n",
      "        [0.0714, 0.0892],\n",
      "        [0.0324, 0.0525],\n",
      "        [0.0821, 0.0737],\n",
      "        [0.1635, 0.1561],\n",
      "        [0.0105, 0.0201],\n",
      "        [0.6450, 0.4702],\n",
      "        [0.1030, 0.0693],\n",
      "        [1.2106, 0.6860],\n",
      "        [0.0350, 0.0467],\n",
      "        [0.4238, 0.4976],\n",
      "        [0.1999, 0.1039],\n",
      "        [0.1119, 0.0670]])}\n",
      "\n",
      "y = (tensor([[4.3730e-03, 1.9010e-03],\n",
      "        [9.0600e-03, 6.6760e-03],\n",
      "        [2.9135e-01, 3.7479e-01],\n",
      "        [2.5940e-01, 4.0436e-01],\n",
      "        [1.3256e-01, 3.0041e-02],\n",
      "        [8.0585e-02, 1.4019e-01],\n",
      "        [8.8215e-02, 8.7738e-02],\n",
      "        [9.5370e-03, 8.2016e-02],\n",
      "        [1.2494e-01, 2.2082e-01],\n",
      "        [8.9169e-02, 8.1539e-02],\n",
      "        [9.4891e-02, 4.8637e-02],\n",
      "        [1.6262e-02, 1.4767e-02],\n",
      "        [2.2936e-01, 3.1567e-01],\n",
      "        [3.4809e-02, 3.6716e-02],\n",
      "        [1.7119e-01, 3.4094e-01],\n",
      "        [6.1989e-02, 1.7166e-02],\n",
      "        [2.0409e-01, 4.0913e-01],\n",
      "        [9.5100e-04, 3.3837e-02],\n",
      "        [1.3924e-01, 3.3999e-01],\n",
      "        [2.5654e-01, 7.4911e-01],\n",
      "        [3.9094e-02, 2.4260e-03],\n",
      "        [1.8358e-01, 3.5667e-01],\n",
      "        [2.6703e-02, 1.0490e-02],\n",
      "        [1.5211e-01, 3.9577e-01],\n",
      "        [1.3285e+00, 1.3199e+00],\n",
      "        [3.3855e-02, 4.5776e-02],\n",
      "        [2.6703e-02, 4.7207e-02],\n",
      "        [0.0000e+00, 0.0000e+00],\n",
      "        [1.0729e-01, 7.2956e-02],\n",
      "        [2.6130e-01, 3.8429e-01],\n",
      "        [7.4863e-02, 4.0531e-02],\n",
      "        [1.0920e-01, 1.6355e-01],\n",
      "        [9.1553e-02, 7.2479e-02],\n",
      "        [2.1648e-01, 1.9360e-01],\n",
      "        [0.0000e+00, 0.0000e+00],\n",
      "        [1.4310e-03, 1.9070e-03],\n",
      "        [3.0041e-01, 1.9073e-01],\n",
      "        [4.4400e-04, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00],\n",
      "        [2.7609e-01, 1.7691e-01],\n",
      "        [4.7700e-04, 1.4310e-03],\n",
      "        [6.8188e-02, 8.6784e-02],\n",
      "        [0.0000e+00, 0.0000e+00],\n",
      "        [4.2677e-01, 3.6335e-01],\n",
      "        [1.7166e-02, 3.3379e-02],\n",
      "        [4.3345e-01, 1.8358e-01],\n",
      "        [9.0122e-02, 1.7023e-01],\n",
      "        [1.3351e-02, 2.8610e-03],\n",
      "        [3.5763e-02, 3.1471e-02],\n",
      "        [2.6703e-02, 2.3365e-02],\n",
      "        [2.0218e-01, 2.8706e-01],\n",
      "        [9.5400e-04, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00],\n",
      "        [3.7003e-01, 8.3733e-01],\n",
      "        [0.0000e+00, 4.7700e-04],\n",
      "        [4.7875e-01, 3.7813e-01],\n",
      "        [5.6267e-02, 7.7248e-02],\n",
      "        [1.1197e-01, 1.3346e-01],\n",
      "        [6.9618e-02, 6.4373e-02],\n",
      "        [2.3680e+00, 1.5306e+00],\n",
      "        [9.1250e-03, 1.3877e-02],\n",
      "        [8.8692e-02, 2.9087e-02],\n",
      "        [2.6035e-01, 2.2364e-01],\n",
      "        [6.1989e-02, 3.6240e-02]]), None)\n",
      "\n",
      "sizes of x =\n",
      "\tencoder_cat = torch.Size([64, 30, 0])\n",
      "\tencoder_cont = torch.Size([64, 30, 1])\n",
      "\tencoder_target = torch.Size([64, 30])\n",
      "\tencoder_lengths = torch.Size([64])\n",
      "\tdecoder_cat = torch.Size([64, 2, 0])\n",
      "\tdecoder_cont = torch.Size([64, 2, 1])\n",
      "\tdecoder_target = torch.Size([64, 2])\n",
      "\tdecoder_lengths = torch.Size([64])\n",
      "\tdecoder_time_idx = torch.Size([64, 2])\n",
      "\tgroups = torch.Size([64, 1])\n",
      "\ttarget_scale = torch.Size([64, 2])\n"
     ]
    }
   ],
   "source": [
    "# convert the dataset to a dataloader\n",
    "dataloader = dataset.to_dataloader(batch_size=64)\n",
    "\n",
    "# and load the first batch\n",
    "x, y = next(iter(dataloader))\n",
    "print(\"x =\", x)\n",
    "print(\"\\ny =\", y)\n",
    "print(\"\\nsizes of x =\")\n",
    "for key, value in x.items():\n",
    "    print(f\"\\t{key} = {value.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac7ff10e-412e-40a3-9720-1758cc0198e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpa1/anaconda3/envs/weather/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/home/lpa1/anaconda3/envs/weather/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Output(prediction=tensor([[0.0227, 0.0241],\n",
       "        [0.2367, 0.2423],\n",
       "        [0.0653, 0.0677],\n",
       "        [0.0144, 0.0150],\n",
       "        [0.1215, 0.1245],\n",
       "        [0.0281, 0.0301],\n",
       "        [0.0991, 0.1013],\n",
       "        [0.2239, 0.2306],\n",
       "        [0.0947, 0.0971],\n",
       "        [0.0888, 0.0955],\n",
       "        [0.0503, 0.0531],\n",
       "        [0.1831, 0.1872],\n",
       "        [0.0883, 0.0917],\n",
       "        [0.3705, 0.3762],\n",
       "        [0.1139, 0.1183],\n",
       "        [0.0786, 0.0826],\n",
       "        [0.0332, 0.0354],\n",
       "        [0.0796, 0.0850],\n",
       "        [0.0092, 0.0098],\n",
       "        [1.3202, 1.3486],\n",
       "        [0.0177, 0.0188],\n",
       "        [0.1190, 0.1227],\n",
       "        [0.2087, 0.2183],\n",
       "        [0.3177, 0.3245],\n",
       "        [0.7613, 0.7740],\n",
       "        [0.1023, 0.1052],\n",
       "        [0.1390, 0.1428],\n",
       "        [0.4958, 0.5138],\n",
       "        [0.3739, 0.3884],\n",
       "        [0.2677, 0.2817],\n",
       "        [0.0743, 0.0776],\n",
       "        [0.3849, 0.3918],\n",
       "        [0.1129, 0.1157],\n",
       "        [0.0881, 0.0905],\n",
       "        [0.2541, 0.2583],\n",
       "        [0.0382, 0.0396],\n",
       "        [0.1606, 0.1655],\n",
       "        [0.0040, 0.0043],\n",
       "        [0.0906, 0.0924],\n",
       "        [0.1325, 0.1381],\n",
       "        [0.0633, 0.0669],\n",
       "        [0.1090, 0.1127],\n",
       "        [0.1138, 0.1168],\n",
       "        [0.0247, 0.0260],\n",
       "        [0.0064, 0.0068],\n",
       "        [0.1180, 0.1223],\n",
       "        [0.0081, 0.0085],\n",
       "        [0.1336, 0.1376],\n",
       "        [0.2872, 0.3016],\n",
       "        [0.0962, 0.0983],\n",
       "        [0.0025, 0.0027],\n",
       "        [0.0572, 0.0599],\n",
       "        [0.3733, 0.3820],\n",
       "        [0.0297, 0.0316],\n",
       "        [0.0741, 0.0768],\n",
       "        [0.1533, 0.1563],\n",
       "        [0.0063, 0.0065],\n",
       "        [0.0499, 0.0523],\n",
       "        [0.4765, 0.4950],\n",
       "        [1.0686, 1.0778],\n",
       "        [0.0814, 0.0841],\n",
       "        [0.4278, 0.4372],\n",
       "        [0.0977, 0.1010],\n",
       "        [0.0415, 0.0439]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PFFullyConnectedModel.from_dataset(dataset, input_size=30, output_size=2, hidden_size=1000, n_hidden_layers=4, learning_rate=1e-3)\n",
    "x, y = next(iter(dataloader))\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49a3026d-09d8-4b25-80a3-884b491a956a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hidden_size\":                 1000\n",
       "\"input_size\":                  30\n",
       "\"learning_rate\":               0.001\n",
       "\"log_gradient_flow\":           False\n",
       "\"log_interval\":                -1\n",
       "\"log_val_interval\":            -1\n",
       "\"logging_metrics\":             ModuleList()\n",
       "\"monotone_constaints\":         {}\n",
       "\"n_hidden_layers\":             4\n",
       "\"optimizer\":                   Ranger\n",
       "\"optimizer_params\":            None\n",
       "\"output_size\":                 2\n",
       "\"output_transformer\":          EncoderNormalizer(\n",
       "\tmethod='standard',\n",
       "\tcenter=True,\n",
       "\tmax_length=None,\n",
       "\ttransformation=None,\n",
       "\tmethod_kwargs={}\n",
       ")\n",
       "\"reduce_on_plateau_min_lr\":    1e-05\n",
       "\"reduce_on_plateau_patience\":  1000\n",
       "\"reduce_on_plateau_reduction\": 2.0\n",
       "\"weight_decay\":                0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cae7e1e-0c15-4bb6-9873-8a85567d5e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "# create PyTorch Lighning Trainer with early stopping\n",
    "early_stop_callback = EarlyStopping(monitor=\"lr-Ranger\", min_delta=1e-4, patience=1, verbose=True, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1000,\n",
    "    accelerator=\"auto\",  # run on CPU, if on multiple GPUs, use strategy=\"ddp\"\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=10000,  # 30 batches per epoch\n",
    "    callbacks=[lr_logger],\n",
    "    # callbacks=[early_stop_callback],\n",
    "    logger=TensorBoardLogger(\"lightning_logs\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "643acf1d-3893-4e51-8a54-d90fa6aca8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightning.pytorch.tuner import Tuner\n",
    "# res = Tuner(trainer).lr_find(\n",
    "#     model, train_dataloaders=dataloader, early_stop_threshold=1000.0, max_lr=0.3,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc09ad7e-e01b-48dc-a50d-bb90f55a58aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpa1/anaconda3/envs/weather/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name            | Type                 | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | loss            | SMAPE                | 0      | train\n",
      "1 | logging_metrics | ModuleList           | 0      | train\n",
      "2 | network         | FullyConnectedModule | 4.0 M  | train\n",
      "-----------------------------------------------------------------\n",
      "4.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 M     Total params\n",
      "16.148    Total estimated model params size (MB)\n",
      "15        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/lpa1/anaconda3/envs/weather/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   1%|  | 5/359 [00:00<00:12, 29.10it/s, v_num=24, train_loss_step=1.020, train_loss_epoch=0.941]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lpa1/anaconda3/envs/weather/lib/python3.10/site-packages/lightning/pytorch/loops/training_epoch_loop.py:389: ReduceLROnPlateau conditioned on metric val_loss which is not available but strict is set to `False`. Skipping learning rate update.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|█| 359/359 [00:11<00:00, 30.14it/s, v_num=24, train_loss_step=0.333, train_loss_epoch=0.4"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|█| 359/359 [00:12<00:00, 29.46it/s, v_num=24, train_loss_step=0.333, train_loss_epoch=0.4\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model, train_dataloaders=dataloader ,# val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "433459f8-f458-434b-ac22-ff3ec8609f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 68.2k\n"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, QuantileLoss\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    # dataset\n",
    "    dataloader.dataset,\n",
    "    # architecture hyperparameters\n",
    "    hidden_size=36,\n",
    "    attention_head_size=2,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=6,\n",
    "    # loss metric to optimize\n",
    "    loss=QuantileLoss(),\n",
    "    # logging frequency\n",
    "    log_interval=2,\n",
    "    # optimizer parameters\n",
    "    learning_rate=0.03,\n",
    "    # reduce_on_plateau_patience=4\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b8a6f63-dd79-4045-b359-6ecbcebc4435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TemporalFusionTransformer(\n",
       "  \t\"attention_head_size\":               2\n",
       "  \t\"categorical_groups\":                {}\n",
       "  \t\"causal_attention\":                  True\n",
       "  \t\"dropout\":                           0.1\n",
       "  \t\"embedding_labels\":                  {}\n",
       "  \t\"embedding_paddings\":                []\n",
       "  \t\"embedding_sizes\":                   {}\n",
       "  \t\"hidden_continuous_size\":            6\n",
       "  \t\"hidden_continuous_sizes\":           {}\n",
       "  \t\"hidden_size\":                       36\n",
       "  \t\"learning_rate\":                     0.03\n",
       "  \t\"log_gradient_flow\":                 False\n",
       "  \t\"log_interval\":                      2\n",
       "  \t\"log_val_interval\":                  2\n",
       "  \t\"lstm_layers\":                       1\n",
       "  \t\"max_encoder_length\":                30\n",
       "  \t\"monotone_constaints\":               {}\n",
       "  \t\"optimizer\":                         Ranger\n",
       "  \t\"optimizer_params\":                  None\n",
       "  \t\"output_size\":                       7\n",
       "  \t\"output_transformer\":                EncoderNormalizer(\n",
       "  \t\tmethod='standard',\n",
       "  \t\tcenter=True,\n",
       "  \t\tmax_length=None,\n",
       "  \t\ttransformation=None,\n",
       "  \t\tmethod_kwargs={}\n",
       "  \t)\n",
       "  \t\"reduce_on_plateau_min_lr\":          1e-05\n",
       "  \t\"reduce_on_plateau_patience\":        1000\n",
       "  \t\"reduce_on_plateau_reduction\":       2.0\n",
       "  \t\"share_single_variable_networks\":    False\n",
       "  \t\"static_categoricals\":               []\n",
       "  \t\"static_reals\":                      []\n",
       "  \t\"time_varying_categoricals_decoder\": []\n",
       "  \t\"time_varying_categoricals_encoder\": []\n",
       "  \t\"time_varying_reals_decoder\":        []\n",
       "  \t\"time_varying_reals_encoder\":        ['tp']\n",
       "  \t\"weight_decay\":                      0.0\n",
       "  \t\"x_categoricals\":                    []\n",
       "  \t\"x_reals\":                           ['tp']\n",
       "  (loss): QuantileLoss(quantiles=[0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98])\n",
       "  (logging_metrics): ModuleList(\n",
       "    (0): SMAPE()\n",
       "    (1): MAE()\n",
       "    (2): RMSE()\n",
       "    (3): MAPE()\n",
       "  )\n",
       "  (input_embeddings): MultiEmbedding(\n",
       "    (embeddings): ModuleDict()\n",
       "  )\n",
       "  (prescalers): ModuleDict(\n",
       "    (tp): Linear(in_features=1, out_features=6, bias=True)\n",
       "  )\n",
       "  (static_variable_selection): VariableSelectionNetwork(\n",
       "    (single_variable_grns): ModuleDict()\n",
       "    (prescalers): ModuleDict()\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (encoder_variable_selection): VariableSelectionNetwork(\n",
       "    (single_variable_grns): ModuleDict(\n",
       "      (tp): GatedResidualNetwork(\n",
       "        (resample_norm): ResampleNorm(\n",
       "          (resample): TimeDistributedInterpolation()\n",
       "          (gate): Sigmoid()\n",
       "          (norm): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (fc1): Linear(in_features=6, out_features=6, bias=True)\n",
       "        (elu): ELU(alpha=1.0)\n",
       "        (fc2): Linear(in_features=6, out_features=6, bias=True)\n",
       "        (gate_norm): GateAddNorm(\n",
       "          (glu): GatedLinearUnit(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (fc): Linear(in_features=6, out_features=72, bias=True)\n",
       "          )\n",
       "          (add_norm): AddNorm(\n",
       "            (norm): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (prescalers): ModuleDict(\n",
       "      (tp): Linear(in_features=1, out_features=6, bias=True)\n",
       "    )\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (decoder_variable_selection): VariableSelectionNetwork(\n",
       "    (single_variable_grns): ModuleDict()\n",
       "    (prescalers): ModuleDict()\n",
       "    (softmax): Softmax(dim=-1)\n",
       "  )\n",
       "  (static_context_variable_selection): GatedResidualNetwork(\n",
       "    (fc1): Linear(in_features=36, out_features=36, bias=True)\n",
       "    (elu): ELU(alpha=1.0)\n",
       "    (fc2): Linear(in_features=36, out_features=36, bias=True)\n",
       "    (gate_norm): GateAddNorm(\n",
       "      (glu): GatedLinearUnit(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (fc): Linear(in_features=36, out_features=72, bias=True)\n",
       "      )\n",
       "      (add_norm): AddNorm(\n",
       "        (norm): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (static_context_initial_hidden_lstm): GatedResidualNetwork(\n",
       "    (fc1): Linear(in_features=36, out_features=36, bias=True)\n",
       "    (elu): ELU(alpha=1.0)\n",
       "    (fc2): Linear(in_features=36, out_features=36, bias=True)\n",
       "    (gate_norm): GateAddNorm(\n",
       "      (glu): GatedLinearUnit(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (fc): Linear(in_features=36, out_features=72, bias=True)\n",
       "      )\n",
       "      (add_norm): AddNorm(\n",
       "        (norm): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (static_context_initial_cell_lstm): GatedResidualNetwork(\n",
       "    (fc1): Linear(in_features=36, out_features=36, bias=True)\n",
       "    (elu): ELU(alpha=1.0)\n",
       "    (fc2): Linear(in_features=36, out_features=36, bias=True)\n",
       "    (gate_norm): GateAddNorm(\n",
       "      (glu): GatedLinearUnit(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (fc): Linear(in_features=36, out_features=72, bias=True)\n",
       "      )\n",
       "      (add_norm): AddNorm(\n",
       "        (norm): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (static_context_enrichment): GatedResidualNetwork(\n",
       "    (fc1): Linear(in_features=36, out_features=36, bias=True)\n",
       "    (elu): ELU(alpha=1.0)\n",
       "    (fc2): Linear(in_features=36, out_features=36, bias=True)\n",
       "    (gate_norm): GateAddNorm(\n",
       "      (glu): GatedLinearUnit(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (fc): Linear(in_features=36, out_features=72, bias=True)\n",
       "      )\n",
       "      (add_norm): AddNorm(\n",
       "        (norm): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lstm_encoder): LSTM(36, 36, batch_first=True)\n",
       "  (lstm_decoder): LSTM(36, 36, batch_first=True)\n",
       "  (post_lstm_gate_encoder): GatedLinearUnit(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (fc): Linear(in_features=36, out_features=72, bias=True)\n",
       "  )\n",
       "  (post_lstm_gate_decoder): GatedLinearUnit(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (fc): Linear(in_features=36, out_features=72, bias=True)\n",
       "  )\n",
       "  (post_lstm_add_norm_encoder): AddNorm(\n",
       "    (norm): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (post_lstm_add_norm_decoder): AddNorm(\n",
       "    (norm): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (static_enrichment): GatedResidualNetwork(\n",
       "    (fc1): Linear(in_features=36, out_features=36, bias=True)\n",
       "    (elu): ELU(alpha=1.0)\n",
       "    (context): Linear(in_features=36, out_features=36, bias=False)\n",
       "    (fc2): Linear(in_features=36, out_features=36, bias=True)\n",
       "    (gate_norm): GateAddNorm(\n",
       "      (glu): GatedLinearUnit(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (fc): Linear(in_features=36, out_features=72, bias=True)\n",
       "      )\n",
       "      (add_norm): AddNorm(\n",
       "        (norm): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (multihead_attn): InterpretableMultiHeadAttention(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (v_layer): Linear(in_features=36, out_features=18, bias=True)\n",
       "    (q_layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=36, out_features=18, bias=True)\n",
       "    )\n",
       "    (k_layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=36, out_features=18, bias=True)\n",
       "    )\n",
       "    (attention): ScaledDotProductAttention(\n",
       "      (softmax): Softmax(dim=2)\n",
       "    )\n",
       "    (w_h): Linear(in_features=18, out_features=36, bias=False)\n",
       "  )\n",
       "  (post_attn_gate_norm): GateAddNorm(\n",
       "    (glu): GatedLinearUnit(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (fc): Linear(in_features=36, out_features=72, bias=True)\n",
       "    )\n",
       "    (add_norm): AddNorm(\n",
       "      (norm): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pos_wise_ff): GatedResidualNetwork(\n",
       "    (fc1): Linear(in_features=36, out_features=36, bias=True)\n",
       "    (elu): ELU(alpha=1.0)\n",
       "    (fc2): Linear(in_features=36, out_features=36, bias=True)\n",
       "    (gate_norm): GateAddNorm(\n",
       "      (glu): GatedLinearUnit(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (fc): Linear(in_features=36, out_features=72, bias=True)\n",
       "      )\n",
       "      (add_norm): AddNorm(\n",
       "        (norm): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_output_gate_norm): GateAddNorm(\n",
       "    (glu): GatedLinearUnit(\n",
       "      (fc): Linear(in_features=36, out_features=72, bias=True)\n",
       "    )\n",
       "    (add_norm): AddNorm(\n",
       "      (norm): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=36, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "932ac99e-0f1f-4565-bcdf-2eb15b314ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.3748e-02,  2.2495e-02],\n",
       "        [ 9.8031e-02,  1.0646e-01],\n",
       "        [ 1.0351e-01,  3.6386e-02],\n",
       "        [-3.5438e-05,  6.2159e-03],\n",
       "        [ 8.0782e-02,  8.4940e-02],\n",
       "        [ 4.0815e-03,  4.4085e-03],\n",
       "        [ 1.9856e-01,  1.7889e-01],\n",
       "        [ 3.7247e-03,  4.1513e-03],\n",
       "        [ 6.0195e-03,  8.8222e-03],\n",
       "        [ 6.0095e-03,  3.0140e-02],\n",
       "        [-3.0226e-03, -4.0457e-03],\n",
       "        [ 4.4956e-01,  3.5594e-01],\n",
       "        [ 5.2398e-01,  5.2683e-01],\n",
       "        [ 5.0546e-01,  4.8624e-01],\n",
       "        [ 1.2502e-02,  3.8954e-02],\n",
       "        [ 3.1226e-03,  4.3975e-03],\n",
       "        [ 8.9126e-03,  2.6478e-03],\n",
       "        [ 1.0307e+00,  1.3347e+00],\n",
       "        [ 8.2471e-04,  2.1327e-03],\n",
       "        [ 1.0463e+01,  1.5974e+01],\n",
       "        [ 1.2068e-01,  1.6947e-01],\n",
       "        [ 7.3796e-02,  1.8571e-01],\n",
       "        [ 5.6835e-02,  6.5052e-02],\n",
       "        [ 5.7857e-01,  5.0337e-01],\n",
       "        [ 2.3188e-01,  2.5130e-01],\n",
       "        [ 1.1520e-01,  1.5184e-02],\n",
       "        [ 1.6728e-01,  3.2459e-01],\n",
       "        [ 4.3851e-02,  3.6964e-02],\n",
       "        [ 1.5614e-01,  2.5027e-01],\n",
       "        [ 2.7304e-02,  5.4323e-03],\n",
       "        [ 1.5179e-02,  8.6931e-03],\n",
       "        [ 1.0056e-01,  7.8433e-02],\n",
       "        [ 1.3139e-01,  6.4166e-02],\n",
       "        [ 1.7874e-01,  2.3976e-01],\n",
       "        [ 5.2021e-01,  5.2058e-01],\n",
       "        [ 2.9274e-03,  1.6944e-03],\n",
       "        [ 7.8799e-02,  1.7184e-01],\n",
       "        [ 4.1867e-03,  9.5774e-04],\n",
       "        [ 1.5902e-02,  2.5305e-03],\n",
       "        [ 1.1059e-02,  3.4887e-02],\n",
       "        [ 1.2589e-02,  1.4818e-02],\n",
       "        [ 1.1286e-02,  2.8559e-02],\n",
       "        [ 7.3751e-02,  5.7912e-02],\n",
       "        [ 2.0544e-01,  1.8622e-01],\n",
       "        [ 3.6931e-03,  1.6851e-02],\n",
       "        [ 1.6642e-01,  1.7339e-01],\n",
       "        [ 4.5313e-04,  5.9566e-04],\n",
       "        [ 2.9390e-03,  2.2037e-02],\n",
       "        [ 6.1939e-02,  9.4220e-02],\n",
       "        [ 1.6845e-01,  1.0002e-01],\n",
       "        [ 1.0678e-03,  6.6374e-04],\n",
       "        [ 1.1308e-02,  2.4206e-02],\n",
       "        [ 6.9517e-02,  7.2918e-02],\n",
       "        [ 3.3355e-03,  9.2341e-02],\n",
       "        [ 8.7916e-02,  2.3658e-02],\n",
       "        [ 6.9610e-02,  7.9536e-02],\n",
       "        [ 5.2039e-02,  3.1771e-02],\n",
       "        [ 9.0218e-02,  2.0067e-01],\n",
       "        [ 8.5879e-01,  5.1438e-01],\n",
       "        [ 1.1611e+00,  1.2227e+00],\n",
       "        [ 2.7983e-01,  2.4374e-01],\n",
       "        [ 7.4940e-01,  8.9571e-01],\n",
       "        [ 3.6673e-01,  2.5576e-01],\n",
       "        [ 4.5589e-02,  2.2155e-02]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = model(x)\n",
    "# x\n",
    "o.prediction\n",
    "# model.transform_output(o.prediction,x['target_scale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47829217-d003-4b4c-830c-a568313220b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.5272e-02, 2.2411e-02],\n",
       "         [1.0014e-01, 1.1110e-01],\n",
       "         [1.0300e-01, 3.6240e-02],\n",
       "         [0.0000e+00, 0.0000e+00],\n",
       "         [8.6784e-02, 8.2493e-02],\n",
       "         [-0.0000e+00, -0.0000e+00],\n",
       "         [2.0218e-01, 1.8311e-01],\n",
       "         [2.8610e-03, 8.1060e-03],\n",
       "         [3.3380e-03, 1.1444e-02],\n",
       "         [5.2060e-03, 3.3350e-02],\n",
       "         [0.0000e+00, 0.0000e+00],\n",
       "         [4.4203e-01, 3.4952e-01],\n",
       "         [5.0020e-01, 5.2166e-01],\n",
       "         [4.9305e-01, 4.7493e-01],\n",
       "         [1.4305e-02, 4.2915e-02],\n",
       "         [2.8950e-03, 4.3430e-03],\n",
       "         [8.5830e-03, 1.9070e-03],\n",
       "         [1.1220e+00, 1.3724e+00],\n",
       "         [0.0000e+00, 0.0000e+00],\n",
       "         [1.0683e+01, 1.5770e+01],\n",
       "         [1.1921e-01, 1.7738e-01],\n",
       "         [7.5340e-02, 1.8501e-01],\n",
       "         [5.1022e-02, 6.1989e-02],\n",
       "         [5.9175e-01, 5.3215e-01],\n",
       "         [2.9659e-01, 2.5988e-01],\n",
       "         [1.1349e-01, 1.7166e-02],\n",
       "         [1.6792e-01, 3.2800e-01],\n",
       "         [3.1948e-02, 2.8610e-02],\n",
       "         [1.5926e-01, 2.5272e-01],\n",
       "         [2.9655e-02, 2.8520e-03],\n",
       "         [1.5259e-02, 9.0600e-03],\n",
       "         [1.1540e-01, 7.1049e-02],\n",
       "         [1.3786e-01, 6.8631e-02],\n",
       "         [1.8454e-01, 2.3794e-01],\n",
       "         [5.0449e-01, 4.9782e-01],\n",
       "         [0.0000e+00, 1.9070e-03],\n",
       "         [8.6308e-02, 1.8311e-01],\n",
       "         [4.2880e-03, 9.3600e-04],\n",
       "         [1.7166e-02, 7.1530e-03],\n",
       "         [0.0000e+00, 0.0000e+00],\n",
       "         [1.2398e-02, 1.5259e-02],\n",
       "         [0.0000e+00, 0.0000e+00],\n",
       "         [7.2956e-02, 5.7697e-02],\n",
       "         [2.1887e-01, 1.8644e-01],\n",
       "         [3.7750e-03, 1.7225e-02],\n",
       "         [1.7548e-01, 1.7405e-01],\n",
       "         [0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00],\n",
       "         [6.3896e-02, 1.0014e-01],\n",
       "         [1.7023e-01, 9.8228e-02],\n",
       "         [0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00],\n",
       "         [4.3869e-02, 2.8133e-02],\n",
       "         [1.4310e-03, 9.8228e-02],\n",
       "         [8.5831e-02, 2.8610e-02],\n",
       "         [6.8665e-02, 8.2016e-02],\n",
       "         [5.2458e-02, 3.0037e-02],\n",
       "         [8.6354e-02, 1.9854e-01],\n",
       "         [8.4592e-01, 5.0353e-01],\n",
       "         [1.1630e+00, 1.2593e+00],\n",
       "         [2.6751e-01, 2.2960e-01],\n",
       "         [7.4005e-01, 8.8501e-01],\n",
       "         [3.6526e-01, 2.5320e-01],\n",
       "         [4.6730e-02, 2.3365e-02]]),\n",
       " None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0491340-e54e-41ce-89b6-c7219f89dfe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
